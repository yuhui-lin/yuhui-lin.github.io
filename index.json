[{"authors":null,"categories":null,"content":"Yuhui Lin is a software engineer.\n","date":1622505600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622505600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Yuhui Lin is a software engineer.","tags":null,"title":"Yuhui Lin","type":"authors"},{"authors":["Yuhui Lin"],"categories":["Technology"],"content":"Overview ClickHouse can read messages directly from a Kafka topic using the Kafka table engine coupled with a materialized view that fetches messages and pushes them to a ClickHouse target table. Here we provide some examples of reading JSON formatted data from Kafka to Clickhouse mergeTree table.\nCreat Kafka topic kafka-topics --bootstrap-server kafka-broker-1.default.svc.cluster.local:9092 \\ --topic clickhouseTestJson --create --partitions 6 --replication-factor 2  MergeTree Table Here we define a mergeTree table which will be used to store injected data.\nCREATE TABLE IF NOT EXISTS event ( date Date DEFAULT toDate(timestamp, 'UTC') Codec(ZSTD), id Int64 Codec(Gorilla, LZ4), timestamp_1min UInt64 DEFAULT (floor(timestamp/60) * 60) Codec(DoubleDelta, LZ4), message LowCardinality(String), timestamp UInt64 Codec(DoubleDelta, LZ4), measure_int UInt64 Codec(Gorilla, LZ4), measure_float Float64 Codec(Gorilla, LZ4), measure_string String Codec(ZSTD) ) ENGINE = ReplicatedMergeTree( '/clickhouse/tables/{shard}/default/customer', '{replica}' ) PARTITION BY toStartOfMonth(date) ORDER BY ( id, timestamp_1min )  Kafka engine with JSONEachRow { \u0026quot;id\u0026quot;: \u0026quot;123\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;test\u0026quot;, \u0026quot;timestamp\u0026quot;: 1234567, \u0026quot;measure_int\u0026quot;: 987 }  If the input data looks like above, we can define a Kafka Engine table with JSONEachRow format.\nCREATE TABLE IF NOT EXISTS json_queue1 ( id Int64, message LowCardinality(String), timestamp UInt64, measure_int UInt64, measure_float Float64, measure_string String ) ENGINE = Kafka SETTINGS kafka_broker_list = 'kafka-broker-1.default.svc.cluster.local:9092', kafka_topic_list = 'clickhouseTestJson', kafka_group_name = 'clickhouseTestJsonGroup1', kafka_format = 'JSONEachRow', kafka_skip_broken_messages = 10000, kafka_max_block_size = 1048576; -- materialized view to automatically move data from Kafka to target table. CREATE MATERIALIZED VIEW json_mv1 TO event AS SELECT id, timestamp, message, measure_int, measure_float, measure_string FROM json_queue1  Kafka engine with JSONAsString { \u0026quot;id\u0026quot;: \u0026quot;123\u0026quot;, \u0026quot;timestamp\u0026quot;: 1234567, \u0026quot;payload\u0026quot; : { \u0026quot;message\u0026quot;: \u0026quot;test\u0026quot;, \u0026quot;measure_string\u0026quot;: \u0026quot;haha\u0026quot; } }  If the input data has nested object, we can use JSONAsString format. The JSON object will be parsed and extracted in the materialized view.\nCREATE TABLE IF NOT EXISTS json_queue2 ( all String ) ENGINE = Kafka SETTINGS kafka_broker_list = 'kafka-broker-1.default.svc.cluster.local:9092', kafka_topic_list = 'clickhouseTestJson', kafka_group_name = 'clickhouseTestJsonGroup2', kafka_format = 'JSONAsString', kafka_skip_broken_messages = 10000, kafka_max_block_size = 1048576; -- materialized view to automatically move data from Kafka to target table. -- Find more ways to extrac values from JSON: https://clickhouse.com/docs/en/sql-reference/functions/json-functions/ CREATE MATERIALIZED VIEW json_mv2 TO event AS SELECT JSONExtract(all, 'id', 'Int64') AS id, JSONExtract(all, 'timestamp', 'Int64') AS timestamp, JSONExtractString(all, 'payload', 'message') AS message, JSONExtractString(all, 'payload', 'measure_string') AS measure_string, FROM json_queue2  ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"c9cebbf6691d942770f4fc4d910fd5dc","permalink":"https://yuhui-lin.github.io/post/2021-06-01_clickhouse-json/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/post/2021-06-01_clickhouse-json/","section":"post","summary":"Clickhouse, JSON, Kafka Engine","tags":["Clickhouse","Kafka","JSON"],"title":"Injecting JSON-formatted data into Clickhouse via Kafka Engine","type":"post"},{"authors":["Yuhui Lin"],"categories":["Technology"],"content":"To download a private repository hosted in Gitlab/Github with GO Module, we can change the url to add git user and access token:\ngit config --global url.\u0026quot;https://${GIT_USER}:${GIT_TOKEN}@gitlab.com/\u0026quot;.insteadOf \u0026quot;https://gitlab.com/\u0026quot;  In Gitlab CI/CD, it should be https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.com/.\nHowever, if the private respository is under a Gitlab subgroup, eg, gitlab.com/company/groupName/repository, the above approach doesn\u0026rsquo;t work. The following approach with netrc will fix it:\nprintf \u0026quot;machine gitlab.com\\n\\ login ${GIT_USER}\\n\\ password ${GIT_TOKEN}\\n\u0026quot;\\ \u0026gt;\u0026gt; /root/.netrc go env -w GOPRIVATE=gitlab.com/company  ","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"cd8758a9a8255231ada4c04d9e214e48","permalink":"https://yuhui-lin.github.io/post/2020-04-20_golang-gitlab-subgroup/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/post/2020-04-20_golang-gitlab-subgroup/","section":"post","summary":"GO Module, Gitlab subgroup","tags":["Gitlab","GO Module","docker","netrc"],"title":"Downloading private repo under Gitlab subgroup using GO Module","type":"post"},{"authors":["Yuhui Lin"],"categories":["blog"],"content":"{% assign lvl = page.url | append:\u0026lsquo;X\u0026rsquo; | split:'/' | size %} {% capture relative %}{% for i in (3..lvl) %}../{% endfor %}{% endcapture %}\n svg { display: block; //margin: 0 auto; margin-left: -120px; }  Overview As an international student in Canada, I found I am only familiar with a few number of TV show/movie stars. As a result, I decided to write a post concerning the relationship between tv/movie celebrities. Instead of focusing on the rumors or hearsay on gossip magzines, this post is more about working relationships and career achievements.\nI get the top 200 celebrities from IMDb Most Popular Females/Males on Aug 2017. This ranking is based on IMDb STARmeter which does not mean the acting skills of the stars but the level of public interest in the person. The working relationships are extracted from IMDb Datasets located in the AWS S3 bucket.\nAll source code has been uploaded to this github repository.\nStrongly connected celebrities Working relationship can be easily demonstrated by graphs. In this post, I use D3.js and Typescript to draw interactively Force-Directed Graph. D3.js is a really cool visualization tool which can be hosted in github.io with Jekyll Blog. while learning Javascript, I found Typescript is rather interesting. It provides not only plenty of object-oriented syntactic sugars, but also static analysis which helps me learn D3.js faster.\nIn the following graph, I tried to find strongly connected celebrities' groups by applying a classic community detection algorithm - Louvain Method. This algorithm divides the top celebrities into groups such that each set of person is densely connected internally and sparsely connected between groups.\nTV stars\u0026rsquo;relationship network:  (You can click-and-drag the nodes around. The name of celebrities should emerge when hovering over the circles. Different groups are distinguished by different color. The radius of circles stand for their number of connections.)\nFor TV show stars, there are a few densely connected groups:\nGame of Throne:\n Emilia Clarke Nikolaj Coster-Waldau Peter Dinklage Lena Headey Kit Harington Aidan Gillen Rose Leslie Sophie Turner Natalie Dormer  Vikings:\n Katheryn Winnick Travis Fimmel  Riverdale:\n Cole Sprouse Mädchen Amick  Big Little Lies:\n Shailene Woodley Reese Witherspoon Shailene Woodley  Guardians of the Galaxy:\n Chris Pratt Zoe Saldana  Movie stars\u0026rsquo;relationship network: \nFor movie stars, they are more likely to collaborate with varous of people.\nCenters Among Stars In this section, I am going to compare the \u0026ldquo;degree\u0026rdquo; of top celebrities.\nReference  inspired by 别开枪，我不是狗仔——数据剖析明星关系    bundle.draw('movie', 'movie', \"{{ relative }}misc/IMDb_celebrities/output/graph_200.json\"); bundle.draw('tv', 'tvEps', \"{{ relative }}misc/IMDb_celebrities/output/graph_200.json\");  ","date":1505001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505001600,"objectID":"1b3e79487ce5b6e6619e472522677446","permalink":"https://yuhui-lin.github.io/post/2017-09-10_imdb/","publishdate":"2017-09-10T00:00:00Z","relpermalink":"/post/2017-09-10_imdb/","section":"post","summary":"Analyze relationship between tv/movie celebrities.","tags":["data analysis","IMDb Database","data analysis","d3.js","typescript"],"title":"Analyzing IMDb Top Celebrities Collaborative Relationship","type":"post"},{"authors":["Yuhui Lin"],"categories":["blog"],"content":"Overview According to Wikipedia, serialization is:\n Serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment).\n Serialization is a pervasive activity in a lot programs, and a common source of memory inefficiency, with lots of temporary data structures needed to parse and represent data, and inefficient allocation patterns and locality.\nThere are several classic data serialization formats, such as JSON, XML, CVS/TVS, Python Pickle, et. Among them, JSON is the most popular format for data serialization, and it has the following features (from this post):\n (Mostly) human readable code, similar to XML/CSV/TSV format. Very simple and straightforward specification. Widespread support: not only does every programming language or IDE come with JSON support, but also many web services APIs offer JSON as a means of data interchange.  As people utilize more complex architecture, serialization must meets several requirements: flexibililty, ability to grow, latency and still stay simple. This is when the drawbacks of classic formats start to hurt:\n lack of strict protocol description you have to maintain both server and client side code the size of human-readable files are larger than binary files lack of backward/forward compatibility  This article is going to introduce some wildly used binary data serialization formats:\n    Formats Features     BSON, MessagePack - Do not require 'Schema' or compiling\n- easy to use\n- Best drop-in replacements if JSON is already used\n- have simple to implement specifications\n- less space efficient    Protocol Buffer, Thrift - mature and wilely used systems\n- best support for multiple languags and cross platform\n- accompanied by good RPC frameworks: gRPC/Thrift    Apache Avro - associated with hadoop/spark ecosystem\n- no compile stage because schema is embedded in the header of messages\n- use JSON as IDL to describe message format\n   Cap’n Proto, FlatBuffers - evolutions of protobuf/thrift\n- require no encoding/decoding stage\n- suitable for mobile/game/VR applications\n    Data Formats BSON BSON, short for Binary JSON, is best known as the primary data rep­res­ent­a­tion for Mon­goDB. It is designed for fast in-memory manipulation and support in-place updating.\nMessagePack MessagePack (MsgPack) is effectively JSON, but with efficient binary encoding. Like JSON, there is no or schemas, which depending on your application can be either be a pro or a con. But, if you are already streaming JSON via an API or using it for storage, then MessagePack can be a drop-in replacement. With a simple specification, MessagePack is supported by over 50 programming languages and environments.\nMessagePack is originally designed for network communication while BSON is designed for storages. As MessagePack\u0026rsquo;s format is less verbose than BSON, MessagePack has smaller encoded file than BSON. As a result, MsgPack is also an extremely good methods to send small messages on wire. Besides, MsgPack has RPC support, static type-checking APIs and streaming APIs.\nProtocol Buffer Protocol Buffer (Protobuf) was originally designed by Google at around 2001, and the second version (proto2) has been open-sourced since 2008. As of today, Protobuf is not only the glue to all Google services, but also an battle tested, very stable, well trusted system.\nThe typical steps of using Protobuf is like: When you run the protocol buffer compiler on a .proto, the compiler generates the code in your chosen language you\u0026rsquo;ll need to work with the message types you\u0026rsquo;ve described in the file, including getting and setting field values, serializing your messages to an output stream, and parsing your messages from an input stream.\nProtocol Buffers language version 3 (aka proto3) was released at Jul 2016, which is not compatible to previous version. Proto3 simplifies the protocol buffer language, both for ease of use and to make it available in a wider range of programming languages: our current release lets you generate protocol buffer code in Java, C++, Python, Java Lite, Ruby, JavaScript, Objective-C, and C#.\nApache Thrift Apache Thrift was first designed internally by Facebook and donated to Apache Foundation afterwards. It has a data serialization format similar to Protobuf, as well as a build-in RFC (remote procedure call) framework.\nThrift is good at building services that work efficiently and seamlessly between C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, OCaml and Delphi and other languages.\n   Apache Avro Apache Avro is a language-neutral data serialization system. The project was created by Doug Cutting (the creator of Hadoop) to address the major downside of Hadoop Writables: lack of language portability.\nAvro provides functionality similar to systems such as Thrift, Protocol Buffers, etc. Avro differs from these systems in the following fundamental aspects:\n Avro data is described using a language-independent schema. However, unlike in the other systems, code generation is optional in Avro, which means you can read and write data that conforms to a given schema even if your code has not seen that particular schema before. To achieve this, Avro assumes that the schema is always present at both read and write time. Avro schemas are defined with JSON . This facilitates implementation in languages that already have JSON libraries. Dynamic typing: Avro does not require that code be generated. Data is always accompanied by a schema that permits full processing of that data without code generation, static datatypes, etc. This facilitates construction of generic data-processing systems and languages. Untagged data: Since the schema is present when data is read, considerably less type information need be encoded with data, resulting in smaller serialization size. No manually-assigned field IDs: When a schema changes, both the old and new schema are always present when processing data, so differences may be resolved symbolically, using field names.  Apache Avro supports languages like C, C++, C#, Go, Haskell, Java, Perl, PHP, Python, Ruby, Scala. What\u0026rsquo;s more important, it works really well with Apache Hadoop/Spark ecosystem.\nCap’n Proto Cap’n Proto is an fast data interchange format and capability-based RPC system. The main selling point of Cap’n Proto is that there is no encoding/decoding step. In other words, Cap’n Proto has the ability to read data without first unpacking it. It is primarily created at around 2014 by Kenton Varda, who was the primary author of Protocol Buffers version 2, which is the version that Google released open source.\nFlatBuffers FlatBuffers is an open source project from Google created mainly by Wouter van Oortmerssen. It is an evolution of protocol buffers that includes object metadata. FlatBuffers follows similar design principles to Cap’n Proto, which is that data should be structured the same way in-memory and on the wire, thus avoiding costly encode/decode steps.\nNo matter how similar they are, there is one scenario where flatbuffers handles better than Cap’n Proto: FlatBuffers, like Protobuf, has the ability to leave out arbitrary fields from a table (better yet, it will automatically leave them out if they happen to have the default value). as data evolves, cases where lots of field get added/removed, or where a lot of fields are essentially empty are extremely common, making this a vital feature for efficiency, and for allowing people to add fields without instantly bloating things.\nIn particular, FlatBuffers focus is on mobile hardware (where memory size and memory bandwidth is even more constrained than on desktop hardware), and applications that have the highest performance needs: games. Facebook has this awesome post about how they improved Facebook\u0026rsquo;s performance on Android with FlatBuffers.\nReferences  https://www.igvita.com/2011/08/01/protocol-buffers-avro-thrift-messagepack/ Data Serialization Comparison: JSON, YAML, BSON, MessagePack Protocol Buffer vs Thrift vs Avro Improving Facebook\u0026rsquo;s performance on Android with FlatBuffers  ","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"542a62b67b00fd42166bbf7020e5a3ea","permalink":"https://yuhui-lin.github.io/post/2017-08-01_data-serialization/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/post/2017-08-01_data-serialization/","section":"post","summary":"Introduction to multiple data serialization methods.","tags":["serialization"],"title":"Data Serialization - JSON, BSON, MessengePack, Protocol Buffer, Thrift, Avro, Cap’n Proto, FlatBuffers","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://yuhui-lin.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]